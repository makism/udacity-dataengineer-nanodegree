{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Udacity Data Engineering Capstone Project**<br/>\n",
    "Avraam Marimpis <avraam.marimpis@gmail.com>, October 2020\n",
    "\n",
    "- - -\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('config/')\n",
    "sys.path.append('common/')\n",
    "\n",
    "import config\n",
    "import data as cnf_data\n",
    "import aws_dwh\n",
    "import preprocess_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as fn\n",
    "import pyspark.sql.types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    \"\"\" Helper utility function to perform `union` on multiple Dataframes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs: list\n",
    "        A list of pyspark.sql.Dataframes.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: pyspark.sql.Dataframe\n",
    "        A new Dataframe that is the result of the unification of the given Dataframes.\n",
    "    \"\"\"\n",
    "    return reduce(pyspark.sql.DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local AWS credentials and settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwh = aws_dwh.parse_dwh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add AWS/S3 JARs to Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config.APP_DEV:\n",
    "    spark.stop()\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "                .appName(\"my_app\") \\\n",
    "                .config('spark.sql.codegen.wholeStage', False) \\\n",
    "                .config(\"spark.driver.extraClassPath\", \"/home/vagrant/opt/libs/aws-java-sdk-1.7.4.jar\") \\\n",
    "                .config(\"spark.jars\", \"/home/vagrant/opt/libs/hadoop-aws-2.7.2.jar\") \\\n",
    "                .getOrCreate()\n",
    "    \n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", dwh['aws']['access_key_id'])\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", dwh['aws']['secret_access_key'])\n",
    "\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "#     spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", f\"s3.{dwh['aws']['region']}.amazonaws.com\")\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets \"Air Quality\", \"US droughts\" and \"Global Temperatures\" are read from Parquet for which, we have well-defined the schemas earlier so there's no need to do it again.\n",
    "\n",
    "However, for the dataset \"Wildfires\", we read it from CSV and we can define a simple schema to meet our demands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \"Wildfires\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wildfires = cnf_data.dataset_wildfiles()\n",
    "print(f\"Dataset source: {dataset_wildfires}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = t.StructType([\n",
    "    t.StructField(\"OBJECTID\", t.StringType()),\n",
    "    t.StructField(\"FOD_ID\", t.StringType()),\n",
    "    t.StructField(\"FPA_ID\", t.StringType()),\n",
    "    t.StructField(\"FIRE_CODE\", t.StringType()),\n",
    "    t.StructField(\"FIRE_NAME\", t.StringType()),\n",
    "    t.StructField(\"ICS_209_INCIDENT_NUMBER\", t.StringType()),\n",
    "    t.StructField(\"ICS_209_NAME\", t.StringType()),\n",
    "    t.StructField(\"MTBS_ID\", t.StringType()),\n",
    "    t.StructField(\"MTBS_FIRE_NAME\", t.StringType()),\n",
    "    t.StructField(\"COMPLEX_NAME\", t.StringType()),\n",
    "    t.StructField(\"FIRE_YEAR\", t.IntegerType()),\n",
    "    t.StructField(\"DISCOVERY_DATE\", t.StringType()),\n",
    "    t.StructField(\"DISCOVERY_DOY\", t.StringType()),\n",
    "    t.StructField(\"DISCOVERY_TIME\", t.StringType()),\n",
    "    t.StructField(\"STAT_CAUSE_CODE\", t.FloatType()),\n",
    "    t.StructField(\"STAT_CAUSE_DESCR\", t.StringType()),\n",
    "    t.StructField(\"CONT_DATE\", t.StringType()),\n",
    "    t.StructField(\"CONT_DOY\", t.StringType()),\n",
    "    t.StructField(\"CONT_TIME\", t.StringType()),\n",
    "    t.StructField(\"FIRE_SIZE\", t.FloatType()),\n",
    "    t.StructField(\"FIRE_SIZE_CLASS\", t.StringType()),\n",
    "    t.StructField(\"STATE\", t.StringType()),\n",
    "    t.StructField(\"COUNTY\", t.StringType()),\n",
    "    t.StructField(\"FIPS_CODE\", t.StringType()),\n",
    "    t.StructField(\"FIPS_NAME\", t.StringType()),\n",
    "    t.StructField(\"DISCOVERY_DATE_converted\", t.DateType()),\n",
    "    t.StructField(\"CONT_DATE_converted\", t.DateType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf = spark.read.csv(dataset_wildfires, header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by dropping some unecessary fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_fields = [\n",
    "    'FIRE_CODE', 'FIRE_NAME',\n",
    "    'ICS_209_INCIDENT_NUMBER', 'ICS_209_NAME',\n",
    "    'MTBS_ID', 'MTBS_FIRE_NAME',\n",
    "    'COMPLEX_NAME', 'DISCOVERY_DATE', 'CONT_DATE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf = df_wf.drop(*drop_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_checked, missing_df = preprocess_fn.missing_fields_perc(df_wf, threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(missing_df.columns) == 0:\n",
    "    print(f\"There no missing values in the given Dataframe.\")\n",
    "else:\n",
    "    print(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = preprocess_fn.count_duplicates(df_wf)\n",
    "print(f\"There are {r} rows duplicate (out of {df_wf.count()} total records).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New partition columns\n",
    "part_cols = {\n",
    "    \"part_year\": fn.year(fn.col(\"DISCOVERY_DATE_converted\")),\n",
    "    \"part_month\": fn.month(fn.col(\"DISCOVERY_DATE_converted\"))\n",
    "}\n",
    "\n",
    "for new_col, col_fn in part_cols.items():\n",
    "    df_wf = df_wf.withColumn(new_col, col_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_wf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf = df_wf.na.fill(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cnf_data.DATASET_STORE == \"local\":\n",
    "    !rm -rf {config.ARTIFACTS}/wildfires\n",
    "    df_wf.coalesce(1).write.mode(\"overwrite\").format('json').save(f\"{config.TEMP}/wildfires/\")\n",
    "    !mv {config.TEMP}/wildfires/ {config.ARTIFACTS}/\n",
    "else:\n",
    "    df_wf.coalesce(1).write.mode(\"overwrite\").format('json').save(f\"{dwh['s3']['bucket-1']['FQN']}/wildfires/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \"Air Quality\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_air_quality = cnf_data.dataset_air_quality()\n",
    "print(f\"Dataset source: {dataset_air_quality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aq = spark.read.parquet(dataset_air_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aq.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqi = df_aq.groupby(\"state_name\")\\\n",
    "#             .agg(fn.avg(\"aqi\").alias(\"avg_aqi\"))\\\n",
    "#             .sort(fn.col(\"avg_aqi\").desc())\\\n",
    "#             .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the for columns with missing or no values, and their percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_checked, missing_df = preprocess_fn.missing_fields_perc(df_aq, threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(missing_df.columns) == 0:\n",
    "    print(f\"There no missing values in the given Dataframe.\")\n",
    "else:\n",
    "    print(missing_df.toPandas().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try do identify which states are missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_missing_df = df_aq.groupby(\"state_name\")\\\n",
    "                    .agg(fn.count(fn.when(fn.isnan(\"aqi\") | fn.col(\"aqi\").isNull(), \"aqi\")).alias(\"missing_values\"))\\\n",
    "                    .sort(fn.col(\"missing_values\").desc())\\\n",
    "                    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_missing_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the state of California reports the least."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out the average and overall AQI (Air Quality Index) per state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_per_state = df_aq.groupby(\"state_name\")\\\n",
    "                    .agg(fn.avg(\"aqi\").alias(\"avg_aqi\"))\\\n",
    "                    .sort(fn.col(\"avg_aqi\").desc())\\\n",
    "                    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_per_state.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = preprocess_fn.count_duplicates(df_aq)\n",
    "print(f\"There are {r} rows duplicate (out of {df_aq.count()} total records).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the most important field in our analysis is also one of the impacted one.\n",
    "\n",
    "We could try different methods of imputing (filling in) these missing values; such as the `Imputer` class from the package `pyspark.ml.feature` but the `AQI` is a discrete index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New partition columns\n",
    "part_cols = {\n",
    "    \"part_year\": fn.year(fn.col(\"date_of_last_change\")),\n",
    "    \"part_month\": fn.month(fn.col(\"date_of_last_change\"))\n",
    "}\n",
    "\n",
    "for new_col, col_fn in part_cols.items():\n",
    "    df_aq = df_aq.withColumn(new_col, col_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aq = df_aq.na.fill(-1, subset=[\"method_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aq = df_aq.na.fill(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cnf_data.DATASET_STORE == \"local\":\n",
    "    !rm -rf {config.ARTIFACTS}/airquality\n",
    "#     df_aq.write.mode(\"overwrite\").partitionBy(\"part_year\").csv(\"/tmp/airquality.csv\")\n",
    "#     df_aq.coalesce(1).write.format('json').save('/tmp/airquality')\n",
    "    df_aq.coalesce(1).write.mode(\"overwrite\").format('json').save(f\"{config.TEMP}/airquality/\")\n",
    "    !mv /tmp/airquality {config.ARTIFACTS}/\n",
    "else:\n",
    "    # partitionBy fails, so we have to skip it :-()\n",
    "    #df_aq.write.mode(\"overwrite\").partitionBy(\"part_year\").parquet(f\"{dwh['s3']['bucket-1']['FQN']}/airquality\")\n",
    "#     df_aq.coalesce(1).write.format('json').save(f\"{dwh['s3']['bucket-1']['FQN']}/airquality/\")\n",
    "    df_aq.coalesce(1).write.mode(\"overwrite\").format('json').save(f\"{dwh['s3']['bucket-1']['FQN']}/airquality/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \"US droughts\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset source: /mtgp/UDACITY/artifacts/sample_us_droughts\n"
     ]
    }
   ],
   "source": [
    "dataset_us_drougts = cnf_data.dataset_us_drougts()\n",
    "print(f\"Dataset source: {dataset_us_drougts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_droughts = spark.read.parquet(dataset_us_drougts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['releaseDate',\n",
       " 'FIPS',\n",
       " 'county',\n",
       " 'state',\n",
       " 'NONE',\n",
       " 'D0',\n",
       " 'D1',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'validStart',\n",
       " 'validEnd',\n",
       " 'domStatisticFormatID',\n",
       " 'county_cleaned']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_droughts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIPS</th>\n",
       "      <td>141751</td>\n",
       "      <td>31418.57105769977</td>\n",
       "      <td>16231.042267302484</td>\n",
       "      <td>1001</td>\n",
       "      <td>72153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>county</th>\n",
       "      <td>141751</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Abbeville County</td>\n",
       "      <td>Ziebach County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>141751</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AK</td>\n",
       "      <td>WY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONE</th>\n",
       "      <td>141751</td>\n",
       "      <td>60.9302231378358</td>\n",
       "      <td>47.10116246375515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D0</th>\n",
       "      <td>141751</td>\n",
       "      <td>39.069776861972194</td>\n",
       "      <td>47.10116246283229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>141751</td>\n",
       "      <td>22.504482648842068</td>\n",
       "      <td>40.30043959972282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>141751</td>\n",
       "      <td>12.022394056306913</td>\n",
       "      <td>31.143338089766747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3</th>\n",
       "      <td>141751</td>\n",
       "      <td>5.12338346868</td>\n",
       "      <td>20.88409465365992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D4</th>\n",
       "      <td>141751</td>\n",
       "      <td>1.3942584530056623</td>\n",
       "      <td>10.906642966203608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domStatisticFormatID</th>\n",
       "      <td>141751</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>county_cleaned</th>\n",
       "      <td>141751</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Abbeville</td>\n",
       "      <td>Ziebach</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0                   1                   2  \\\n",
       "summary                count                mean              stddev   \n",
       "FIPS                  141751   31418.57105769977  16231.042267302484   \n",
       "county                141751                None                None   \n",
       "state                 141751                None                None   \n",
       "NONE                  141751    60.9302231378358   47.10116246375515   \n",
       "D0                    141751  39.069776861972194   47.10116246283229   \n",
       "D1                    141751  22.504482648842068   40.30043959972282   \n",
       "D2                    141751  12.022394056306913  31.143338089766747   \n",
       "D3                    141751       5.12338346868   20.88409465365992   \n",
       "D4                    141751  1.3942584530056623  10.906642966203608   \n",
       "domStatisticFormatID  141751                 1.0                 0.0   \n",
       "county_cleaned        141751                None                None   \n",
       "\n",
       "                                     3               4  \n",
       "summary                            min             max  \n",
       "FIPS                              1001           72153  \n",
       "county                Abbeville County  Ziebach County  \n",
       "state                               AK              WY  \n",
       "NONE                               0.0           100.0  \n",
       "D0                                 0.0           100.0  \n",
       "D1                                 0.0           100.0  \n",
       "D2                                 0.0           100.0  \n",
       "D3                                 0.0           100.0  \n",
       "D4                                 0.0           100.0  \n",
       "domStatisticFormatID                 1               1  \n",
       "county_cleaned               Abbeville         Ziebach  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_droughts.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_checked, missing_df = preprocess_fn.missing_fields_perc(df_droughts, threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There no missing values in the given Dataframe.\n"
     ]
    }
   ],
   "source": [
    "if len(missing_df.columns) == 0:\n",
    "    print(f\"There no missing values in the given Dataframe.\")\n",
    "else:\n",
    "    print(missing_df.toPandas().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with `threshold` set to `0.0`, it seems that there are no empty records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6913 rows incomplete (out of 141751 total records).\n"
     ]
    }
   ],
   "source": [
    "r = preprocess_fn.count_duplicates(df_droughts)\n",
    "print(f\"There are {r} rows incomplete (out of {df_droughts.count()} total records).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\"domStatisticFormatID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_droughts = df_droughts.drop(*drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's find out the number of droughts per state and per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_states_year_count = df_droughts.groupby([\"state\", fn.year(fn.col(\"releaseDate\")).alias(\"year\")])\\\n",
    "                    .agg(fn.count(fn.col(\"state\")).alias(\"count\"))\\\n",
    "                    .sort([fn.col(\"state\"), fn.col(\"year\")])\\\n",
    "                    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>year</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AK</td>\n",
       "      <td>2000</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>2001</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AK</td>\n",
       "      <td>2002</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AK</td>\n",
       "      <td>2003</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AK</td>\n",
       "      <td>2004</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>WY</td>\n",
       "      <td>2012</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>WY</td>\n",
       "      <td>2013</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>WY</td>\n",
       "      <td>2014</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>WY</td>\n",
       "      <td>2015</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>WY</td>\n",
       "      <td>2016</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>881 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    state  year  count\n",
       "0      AK  2000     72\n",
       "1      AK  2001     57\n",
       "2      AK  2002     72\n",
       "3      AK  2003     76\n",
       "4      AK  2004     75\n",
       "..    ...   ...    ...\n",
       "876    WY  2012     65\n",
       "877    WY  2013     61\n",
       "878    WY  2014     75\n",
       "879    WY  2015     57\n",
       "880    WY  2016     63\n",
       "\n",
       "[881 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_states_year_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pivot table will summarize the results better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_tbl = pd_states_year_count.pivot(\"state\", \"year\", \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>year</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AK</th>\n",
       "      <td>72.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>164.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>206.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>38.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>161.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "year    2000   2001   2002   2003   2004   2005   2006   2007   2008   2009  \\\n",
       "state                                                                         \n",
       "AK      72.0   57.0   72.0   76.0   75.0   60.0   78.0   60.0   71.0   67.0   \n",
       "AL     164.0  179.0  179.0  178.0  180.0  174.0  168.0  155.0  179.0  168.0   \n",
       "AR     206.0  224.0  200.0  205.0  174.0  186.0  190.0  179.0  182.0  193.0   \n",
       "AZ      38.0   39.0   42.0   38.0   28.0   41.0   38.0   39.0   40.0   37.0   \n",
       "CA     161.0  146.0  175.0  141.0  149.0  157.0  144.0  167.0  148.0  147.0   \n",
       "\n",
       "year    2010   2011   2012   2013   2014   2015   2016  \n",
       "state                                                   \n",
       "AK      56.0   70.0   72.0   70.0   81.0   81.0   69.0  \n",
       "AL     170.0  166.0  168.0  167.0  169.0  175.0  135.0  \n",
       "AR     217.0  193.0  197.0  184.0  214.0  183.0  176.0  \n",
       "AZ      38.0   37.0   49.0   41.0   34.0   48.0   39.0  \n",
       "CA     145.0  150.0  162.0  143.0  133.0  159.0  127.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_tbl.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New partition columns\n",
    "part_cols = {\n",
    "    \"part_year\": fn.year(fn.col(\"releaseDate\")),\n",
    "    \"part_month\": fn.month(fn.col(\"releaseDate\"))\n",
    "}\n",
    "\n",
    "for new_col, col_fn in part_cols.items():\n",
    "    df_droughts = df_droughts.withColumn(new_col, col_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_droughts = df_droughts.na.fill(-1.0, subset=[\"NONE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_droughts = df_droughts.na.fill(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cnf_data.DATASET_STORE == \"local\":\n",
    "    !rm -rf {config.ARTIFACTS}/droughts\n",
    "#     df_droughts.write.mode(\"overwrite\").partitionBy(\"part_year\").parquet(\"/tmp/droughts\")\n",
    "    df_droughts.coalesce(1).write.mode(\"overwrite\").format('json').save(f\"{config.TEMP}/droughts/\")\n",
    "    !mv {config.TEMP}/droughts/ {config.ARTIFACTS}/\n",
    "else:\n",
    "#     df_droughts.write.mode(\"overwrite\").parquet(f\"{dwh['s3']['bucket-1']['FQN']}/droughts\")\n",
    "    df_droughts.coalesce(1).write.mode(\"overwrite\").format('json').save(f\"{dwh['s3']['bucket-1']['FQN']}/droughts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \"Global Temperatures\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_global_temps = cnf_data.dataset_global_temps()\n",
    "print(f\"Dataset source: {dataset_global_temps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temps = spark.read.parquet(dataset_global_temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temps.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temps.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_checked, missing_df = preprocess_fn.missing_fields_perc(df_temps, threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(missing_df.columns) == 0:\n",
    "    print(f\"There no missing values in the given Dataframe.\")\n",
    "else:\n",
    "    print(missing_df.toPandas().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with `threshold` set to `0.0`, it seems that there are no empty records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = preprocess_fn.count_duplicates(df_temps)\n",
    "print(f\"There are {r} rows incomplete (out of {df_temps.count()} total records).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the average temperature per state / per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_states_year_count = df_temps.groupby([\"State\", fn.year(fn.col(\"dt\")).alias(\"year\")])\\\n",
    "                    .agg(fn.avg(fn.col(\"AverageTemperature\")).alias(\"avg_temp\"))\\\n",
    "                    .sort([fn.col(\"State\"), fn.col(\"year\")])\\\n",
    "                    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a pivot table creates a readable summary of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_tbl = pd_states_year_count.pivot(\"State\", \"year\", \"avg_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_tbl.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some exploration, we can come into the conclusion that the `NaN`s, actually represent missing rows for the specific years. So, we will have to insert dummy records, filled with zeros for the states that do not have these records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice lots of missing values; let's try to impute them using the average temperature per state. We could also take the average temperature for each state per month across all the yearly recordings.\n",
    "\n",
    "More sofisticated strategies include, building a linear regression model (although this depends whether or not the colinearity of the variables); and more data sources and observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's fill in the missing records\n",
    "# Please consult the setting `config/data.py:DATESET_FILLIN_DT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cnf_data.DATESET_FILLIN_DT:\n",
    "    all_states = df_temps.select(\"State\").distinct().collect()\n",
    "\n",
    "    max_year, min_year = df_temps.select(fn.year(fn.col(\"dt\")).alias(\"year\"))\\\n",
    "                                .agg(\n",
    "                                    fn.max(fn.col(\"year\")).alias(\"max\"),\n",
    "                                    fn.min(fn.col(\"year\")).alias(\"min\")\n",
    "                                ).collect()[0]\n",
    "\n",
    "    temps_period = pd.period_range(start=min_year, end=max_year).to_native_types().tolist()\n",
    "\n",
    "    new_dfs = []\n",
    "\n",
    "    for state in all_states:\n",
    "        df_state = df_temps.filter(fn.col(\"State\") == state[0])\n",
    "        df_dates = df_state.select(fn.date_format(fn.col(\"dt\"), \"YYYY-MM-dd\").alias(\"dt_formatted\")).sort(fn.col(\"dt_formatted\")).distinct()\n",
    "\n",
    "        state_periods = list(map(lambda row: row['dt_formatted'], df_dates.collect()))\n",
    "\n",
    "        # Find the differences between the complete periods in the dataset, and the periods for the current state.\n",
    "        set_full = set(temps_period)\n",
    "        set_state = set(state_periods)\n",
    "        diff = list(set_full - set_state)\n",
    "\n",
    "        # Generate new rows\n",
    "        make_rows = list(map(lambda dt: [\n",
    "            datetime.datetime.strptime(dt, '%Y-%m-%d').date(),\n",
    "            0.0,\n",
    "            0.0,\n",
    "            state[0],\n",
    "    #         1,\n",
    "    #         1\n",
    "            datetime.datetime.strptime(dt, '%Y-%m-%d').year,\n",
    "            datetime.datetime.strptime(dt, '%Y-%m-%d').month\n",
    "        ], diff)\n",
    "        )\n",
    "        df_new_rows = spark.createDataFrame(make_rows, schema=df_temps.schema)\n",
    "    #     df_temps = df_temps.union(df_new_rows)\n",
    "\n",
    "        new_dfs.append(df_new_rows)\n",
    "\n",
    "    new_df_temps = unionAll(*new_dfs)\n",
    "    new_df_temps = df_temps.union(new_df_temps)\n",
    "\n",
    "    if cnf_data.DATASET_STORE == \"local\":\n",
    "        new_df_temps.write.mode(\"overwrite\").parquet(\"/tmp/filled_temperatures\")\n",
    "        !mv /tmp/filled_temperatures/ {config.ARTIFACTS}/\n",
    "    else:\n",
    "        new_df_temps.write.mode(\"overwrite\").parquet(f\"s3a://{dwh['s3']['init_bucket']}/filled_temperatures\")\n",
    "        \n",
    "    df = new_df_temps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's impute the missing values; you may skip this step if it's taking too long\n",
    "# Please consult the setting `config/data.py:DATASET_IMPUTE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cnf_data.DATASET_IMPUTE:\n",
    "    from pyspark.ml.feature import Imputer\n",
    "    \n",
    "    all_states = df.select(\"State\").distinct().collect()\n",
    "    \n",
    "    new_dfs = []\n",
    "    for state in all_states:\n",
    "        imputer = Imputer(strategy=\"mean\", inputCols=[\"AverageTemperature\"], outputCols=[\"AverageTemperature_imputed\"])\n",
    "        df_state = df.filter(fn.col(\"State\") == state[0])\n",
    "        imputed_state = imputer.fit(df_state).transform(df_state)\n",
    "\n",
    "        new_dfs.append(imputed_state)\n",
    "        \n",
    "    imputed_dfs = unionAll(*new_dfs)\n",
    "    \n",
    "    if cnf_data.DATASET_STORE == \"local\":\n",
    "        !rm -rf {config.ARTIFACTS}/imputed_temperatures/\n",
    "        imputed_dfs.coalesce(1).write.mode(\"overwrite\").format('json').save(f\"{config.TEMP}/imputed_temperatures/\")\n",
    "        !mv {config.TEMP}/imputed_temperatures/ {config.ARTIFACTS}/\n",
    "    else:\n",
    "        imputed_dfs.coalesce(1).write.mode(\"overwrite\").format('json').save(f\"{dwh['s3']['bucket-1']['FQN']}/imputed_temperatures\")\n",
    "                                                                            \n",
    "    df = imputed_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_states_year_count = df.groupby([\"State\", fn.year(fn.col(\"dt\")).alias(\"year\")])\\\n",
    "                    .agg(fn.avg(fn.col(\"AverageTemperature_imputed\")).alias(\"avg_temp\"))\\\n",
    "                    .sort([fn.col(\"State\"), fn.col(\"year\")])\\\n",
    "                    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_tbl = pd_states_year_count.pivot(\"State\", \"year\", \"avg_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_tbl.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New partition columns\n",
    "part_cols = {\n",
    "    \"part_year\": fn.year(fn.col(\"dt\")),\n",
    "    \"part_month\": fn.month(fn.col(\"dt\"))\n",
    "}\n",
    "\n",
    "for new_col, col_fn in part_cols.items():\n",
    "    df = df.withColumn(new_col, col_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing with extreme values so as the JSON dumped, do not exclude these fields\n",
    "\n",
    "df = df.na.fill(-10000.0, subset=[\"AverageTemperatureUncertainty\"])\n",
    "df = df.na.fill(-10000.0, subset=[\"AverageTemperature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cnf_data.DATASET_STORE == \"local\":\n",
    "    !rm -rf {config.ARTIFACTS}/temperatures\n",
    "    df.coalesce(1).write.mode(\"overwrite\").format('json').save(f\"{config.TEMP}/temperatures/\")\n",
    "    !mv {config.TEMP}/temperatures/ {config.ARTIFACTS}/\n",
    "else:\n",
    "    df.coalesce(1).write.mode(\"overwrite\").format('json').save(f\"{dwh['s3']['bucket-1']['FQN']}/temperatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"common/\")\n",
    "import aws_dwh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload JSON Paths to S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\n",
    "    f\"{config.REDSHIFT_JSON_PATHS}\"\n",
    "]\n",
    "\n",
    "for d in dirs:\n",
    "    aws_dwh.upload_to_s3(d, dwh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload local artifacts to S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cnf_data.DATASET_STORE == \"local\":\n",
    "    dirs = [\n",
    "        f\"{config.ARTIFACTS}/wildfires\",\n",
    "        f\"{config.ARTIFACTS}/imputed_temperatures\",\n",
    "        f\"{config.ARTIFACTS}/temperatures\",\n",
    "        f\"{config.ARTIFACTS}/airquality\",\n",
    "        f\"{config.ARTIFACTS}/droughts\"\n",
    "    ]\n",
    "\n",
    "    for d in dirs:\n",
    "        aws_dwh.upload_to_s3(d, dwh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
