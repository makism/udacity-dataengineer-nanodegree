{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Udacity Data Engineering Capstone Project**<br/>\n",
    "Avraam Marimpis <avraam.marimpis@gmail.com>, October 2020\n",
    "\n",
    "- - -\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('config/')\n",
    "sys.path.append('common/')\n",
    "\n",
    "import logger\n",
    "import config\n",
    "import data as cnf_data\n",
    "import aws_dwh\n",
    "import preprocess_fn\n",
    "import redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as fn\n",
    "import pyspark.sql.types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_handler = logger.get_logger(logger_name=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local AWS credentials and settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwh = aws_dwh.parse_dwh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Redshift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(f\"host={dwh['redshift']['host']} dbname=dev port=5439 user=awsuser password={dwh['redshift']['db_pass']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ddl_query(query, print_query=True):\n",
    "    \"\"\" A helper function to execute the given query. It supports error handling and verbose messages.\n",
    "    \n",
    "    This function is to be used only for INSERT, DELETE and CREATE statements.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query: string\n",
    "        The SQL query to execute.\n",
    "    \"\"\"\n",
    "    q = query\n",
    "    \n",
    "    if print_query:\n",
    "        print(q)\n",
    "    \n",
    "    try:\n",
    "        cur.execute(q)\n",
    "    except Exception as err:\n",
    "        log_handler.error(err)\n",
    "        cur.execute(\"ROLLBACK;\")\n",
    "    conn.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Declaring UDF \"f_my_cast\".\n"
     ]
    }
   ],
   "source": [
    "for udf, cmd in redshift.Redshift.UDFs.items():\n",
    "    try:\n",
    "        print(f\"Declaring UDF \\\"{udf}\\\".\")\n",
    "        cur.execute(cmd)\n",
    "    except Exception as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop all tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping table \"stage_wildfires\".\n",
      "\n",
      "            DROP TABLE IF EXISTS public.stage_wildfires;\n",
      "        \n",
      "Dropping table \"stage_airquality\".\n",
      "\n",
      "            DROP TABLE IF EXISTS public.stage_airquality;\n",
      "        \n",
      "Dropping table \"stage_temperatures\".\n",
      "\n",
      "            DROP TABLE IF EXISTS public.stage_temperatures;\n",
      "        \n",
      "Dropping table \"stage_droughts\".\n",
      "\n",
      "            DROP TABLE IF EXISTS public.stage_droughts;\n",
      "        \n",
      "Dropping table \"states_abbrv\".\n",
      "\n",
      "            DROP TABLE IF EXISTS public.states_abbrv;\n",
      "        \n",
      "Dropping table \"states_counties\".\n",
      "\n",
      "            DROP TABLE IF EXISTS public.states_counties CASCADE;\n",
      "        \n",
      "Dropping table \"fips\".\n",
      "\n",
      "            DROP TABLE IF EXISTS public.fips CASCADE;\n",
      "        \n",
      "Dropping table \"wildfires\".\n",
      "\n",
      "            DROP TABLE IF EXISTS public.wildfires;\n",
      "        \n",
      "Dropping table \"droughts\".\n",
      "\n",
      "            DROP TABLE IF EXISTS public.droughts;\n",
      "        \n",
      "Dropping table \"temperatures\".\n",
      "\n",
      "            DROP TABLE IF EXISTS public.temperatures;\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "for tbl, cmd in redshift.Redshift.Drop.items():\n",
    "    print(f\"Dropping table \\\"{tbl}\\\".\")\n",
    "    run_ddl_query(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Staging Table \"Wildfires\"\n",
    "\n",
    "table name: `stage_wildfires`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Create['stage_wildfires']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            CREATE TABLE IF NOT EXISTS public.stage_wildfires (\n",
      "                OBJECTID                 VARCHAR(256),\n",
      "                FOD_ID                   VARCHAR(256),\n",
      "                FPA_ID                   VARCHAR(256),\n",
      "                FIRE_YEAR                VARCHAR(256),\n",
      "                DISCOVERY_DOY            VARCHAR(256),\n",
      "                DISCOVERY_TIME           VARCHAR(256),\n",
      "                STAT_CAUSE_CODE          VARCHAR(256),\n",
      "                STAT_CAUSE_DESCR         VARCHAR(256),\n",
      "                CONT_DOY                 VARCHAR(256),\n",
      "                CONT_TIME                VARCHAR(256),\n",
      "                FIRE_SIZE                VARCHAR(256),\n",
      "                FIRE_SIZE_CLASS          VARCHAR(256),\n",
      "                STATE                    VARCHAR(256),\n",
      "                COUNTY                   VARCHAR(256),\n",
      "                FIPS_CODE                VARCHAR(256),\n",
      "                FIPS_NAME                VARCHAR(256),\n",
      "                DISCOVERY_DATE_converted VARCHAR(256),\n",
      "                CONT_DATE_converted      VARCHAR(256),\n",
      "                part_year                VARCHAR(256),\n",
      "                part_month               VARCHAR(256)\n",
      "            );\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Staging Table \"Air Quality\"\n",
    "table name: `stage_airquality`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Create['stage_airquality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            CREATE TABLE IF NOT EXISTS public.stage_airquality (\n",
      "                state_code          VARCHAR,\n",
      "                county_code         VARCHAR,\n",
      "                site_num            VARCHAR,\n",
      "                parameter_code      INTEGER,\n",
      "                poc                 INTEGER,\n",
      "                latitude            FLOAT,\n",
      "                longitude           FLOAT,\n",
      "                datum               VARCHAR,\n",
      "                parameter_name      VARCHAR,\n",
      "                sample_duration     VARCHAR,\n",
      "                pollutant_standard  VARCHAR, \n",
      "                date_local          VARCHAR,\n",
      "                units_of_measure    VARCHAR,\n",
      "                event_type          VARCHAR,\n",
      "                observation_count   INTEGER,\n",
      "                observation_percent INTEGER,\n",
      "                arithmetic_mean     FLOAT,\n",
      "                first_max_value     FLOAT,\n",
      "                first_max_hour      INTEGER,\n",
      "                aqi                 INTEGER,\n",
      "                method_code         INTEGER,\n",
      "                method_name         VARCHAR,\n",
      "                local_site_name     VARCHAR,\n",
      "                address             VARCHAR,\n",
      "                state_name          VARCHAR,\n",
      "                county_name         VARCHAR,\n",
      "                city_name           VARCHAR,\n",
      "                cbsa_name           VARCHAR,\n",
      "                date_of_last_change VARCHAR,\n",
      "                part_year           INTEGER,\n",
      "                part_month          INTEGER\n",
      "            );\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staging Table \"Droughts\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Create['stage_droughts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            CREATE TABLE IF NOT EXISTS public.stage_droughts (\n",
      "                releaseDate    VARCHAR,\n",
      "                FIPS           VARCHAR,\n",
      "                county         VARCHAR,\n",
      "                state          VARCHAR,\n",
      "                NONE           FLOAT,\n",
      "                D0             FLOAT,\n",
      "                D1             FLOAT,\n",
      "                D2             FLOAT,\n",
      "                D3             FLOAT,\n",
      "                D4             FLOAT,\n",
      "                validStart     VARCHAR,\n",
      "                validEnd       VARCHAR,\n",
      "                county_cleaned VARCHAR,\n",
      "                part_year      INTEGER,\n",
      "                part_month     INTEGER\n",
      "            );\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staging Table \"Temperatures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Create['stage_temperatures']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            CREATE TABLE IF NOT EXISTS public.stage_temperatures (\n",
      "                dt                            VARCHAR,\n",
      "                AverageTemperature            FLOAT,\n",
      "                AverageTemperatureUncertainty FLOAT,\n",
      "                State                         VARCHAR,\n",
      "                AverageTemperature_imputed    FLOAT,\n",
      "                part_year                     INTEGER,\n",
      "                part_month                    INTEGER\n",
      "            );\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest and Stage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Wildfires "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmt = redshift.Redshift.Stage['stage_wildfires']\n",
    "\n",
    "bucket = f\"{dwh['s3']['bucket-1']['name']}\"\n",
    "json = \"part-00000-2457b28a-f8e7-49e8-a34e-4d8d41e9bf6e-c000.json\"\n",
    "\n",
    "q = stmt.format(\n",
    "    bucket,\n",
    "    json,\n",
    "    dwh['aws']['access_key_id'], dwh['aws']['secret_access_key'], \n",
    "    bucket,\n",
    "    dwh['aws']['region']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            COPY public.stage_wildfires FROM 's3://am-capstone-bucket-1/wildfires/part-00000-2457b28a-f8e7-49e8-a34e-4d8d41e9bf6e-c000.json'\n",
      "            ACCESS_KEY_ID 'AKIAXIBBBG5P43B5TWYV'\n",
      "            SECRET_ACCESS_KEY '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'\n",
      "            JSON 's3://am-capstone-bucket-1/json_paths/wildfires.json' COMPUPDATE OFF\n",
      "            REGION 'eu-central-1';\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Air Quality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmt = redshift.Redshift.Stage['stage_airquality']\n",
    "\n",
    "bucket = f\"{dwh['s3']['bucket-1']['name']}\"\n",
    "json = \"part-00000-916aee51-cd33-4dc2-88ad-f0fb96d4d766-c000.json\"\n",
    "\n",
    "q = stmt.format(\n",
    "    bucket,\n",
    "    json,\n",
    "    dwh['aws']['access_key_id'], dwh['aws']['secret_access_key'],\n",
    "    bucket,\n",
    "    dwh['aws']['region']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            COPY public.stage_airquality FROM 's3://am-capstone-bucket-1/airquality/part-00000-916aee51-cd33-4dc2-88ad-f0fb96d4d766-c000.json'\n",
      "            ACCESS_KEY_ID 'AKIAXIBBBG5P43B5TWYV' SECRET_ACCESS_KEY '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'\n",
      "            JSON 's3://am-capstone-bucket-1/json_paths/airquality.json' COMPUPDATE OFF\n",
      "            REGION 'eu-central-1';\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Droughts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmt = redshift.Redshift.Stage['stage_droughts']\n",
    "\n",
    "bucket = f\"{dwh['s3']['bucket-1']['name']}\"\n",
    "json = \"part-00000-5923827d-34e5-49f0-bb14-6a57a0e45c13-c000.json\"\n",
    "\n",
    "q = stmt.format(\n",
    "    bucket,\n",
    "    json,\n",
    "    dwh['aws']['access_key_id'], dwh['aws']['secret_access_key'],\n",
    "    bucket,\n",
    "    dwh['aws']['region']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            COPY public.stage_droughts FROM 's3://am-capstone-bucket-1/droughts/part-00000-5923827d-34e5-49f0-bb14-6a57a0e45c13-c000.json'\n",
      "            ACCESS_KEY_ID 'AKIAXIBBBG5P43B5TWYV'\n",
      "            SECRET_ACCESS_KEY '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'\n",
      "            JSON 's3://am-capstone-bucket-1/json_paths/droughts.json' COMPUPDATE OFF\n",
      "            REGION 'eu-central-1';\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmt = redshift.Redshift.Stage['stage_temperatures']\n",
    "\n",
    "bucket = f\"{dwh['s3']['bucket-1']['name']}\"\n",
    "json = \"part-00000-a68ff967-8bc1-498a-9747-8702401efc06-c000.json\"\n",
    "\n",
    "q = stmt.format(\n",
    "    bucket,\n",
    "    json,\n",
    "    dwh['aws']['access_key_id'], dwh['aws']['secret_access_key'], \n",
    "    bucket,\n",
    "    dwh['aws']['region']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            COPY public.stage_temperatures FROM 's3://am-capstone-bucket-1/temperatures/part-00000-a68ff967-8bc1-498a-9747-8702401efc06-c000.json'\n",
      "            ACCESS_KEY_ID 'AKIAXIBBBG5P43B5TWYV' SECRET_ACCESS_KEY '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'\n",
      "            JSON 's3://am-capstone-bucket-1/json_paths/temperatures.json' COMPUPDATE OFF\n",
      "            REGION 'eu-central-1';\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create facts and dimension tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table FIPS\n",
    "table name: `fips`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Create['fips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            CREATE TABLE IF NOT EXISTS fips (\n",
      "                index     INTEGER IDENTITY(0,1) PRIMARY KEY NOT NULL,\n",
      "                fips_code INTEGER,\n",
      "                fips_name VARCHAR\n",
      "            );\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table States Abbreviations \n",
    "table name: `states_abbr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Create['states_abbrv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            CREATE TABLE IF NOT EXISTS public.states_abbrv (\n",
      "                index  INTEGER IDENTITY(0,1) PRIMARY KEY NOT NULL,\n",
      "                state  VARCHAR,\n",
      "                abbr   VARCHAR(2)\n",
      "            );\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table States and Counties\n",
    "table name: `states_counties`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Create['states_counties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            CREATE TABLE IF NOT EXISTS public.states_counties (\n",
      "                index  INTEGER IDENTITY(0,1) PRIMARY KEY NOT NULL,\n",
      "                state  VARCHAR(2),\n",
      "                county VARCHAR\n",
      "            );\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Wildfires\n",
    "table name: `wildfires`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Create['wildfires']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            CREATE TABLE IF NOT EXISTS wildfires(\n",
      "                OBJECTID          VARCHAR PRIMARY KEY NOT NULL,\n",
      "                FOD_ID            VARCHAR,\n",
      "                FPA_ID            VARCHAR,\n",
      "                FIRE_YEAR         INTEGER,\n",
      "                DISCOVERY_DOY     INTEGER,\n",
      "                DISCOVERY_TIME    INTEGER,\n",
      "                STAT_CAUSE_CODE   INTEGER,\n",
      "                STAT_CAUSE_DESCR  VARCHAR,\n",
      "                CONT_DOY          INTEGER,\n",
      "                CONT_TIME         INTEGER,\n",
      "                FIRE_SIZE         FLOAT,\n",
      "                FIRE_SIZE_CLASS   VARCHAR(1),\n",
      "                STATE_COUNTY      INTEGER REFERENCES public.states_counties(index),\n",
      "                FIPS_ID           INTEGER REFERENCES public.fips(index),\n",
      "                DISCOVERY_DATE    DATE,\n",
      "                CONT_DATE         DATE\n",
      "            );\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Airquality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Create['airquality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            CREATE TABLE IF NOT EXISTS airquality(\n",
      "                STATE_COUNTY      INTEGER REFERENCES public.states_counties(index),\n",
      "                city_name         VARCHAR,\n",
      "                aqi                 INTEGER,\n",
      "                date_of_last_change DATE\n",
      "            );\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Droughts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Create['droughts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            CREATE TABLE IF NOT EXISTS droughts(\n",
      "                releaseDate  DATE,\n",
      "                FIPS_ID      INTEGER REFERENCES public.fips(index),\n",
      "                STATE_COUNTY INTEGER REFERENCES public.states_counties(index),\n",
      "                None         FLOAT,\n",
      "                D0           FLOAT,\n",
      "                D1           FLOAT,\n",
      "                D2           FLOAT,\n",
      "                D3           FLOAT,\n",
      "                D4           FLOAT,\n",
      "                validStart   DATE,\n",
      "                validEnd     DATE\n",
      "            );\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Temperatures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Create['temperatures']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            CREATE TABLE IF NOT EXISTS temperatures(\n",
      "                    dt                 DATE,\n",
      "                    State              VARCHAR,\n",
      "                    AverageTemperature FLOAT\n",
      "            );\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Annual Reports\n",
    "table name: `annual_reports`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Create['annual_reports']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            CREATE TABLE IF NOT EXISTS annual_reports (\n",
      "                state       VARCHAR,\n",
      "                year        INTEGER,\n",
      "                aqi         INTEGER,\n",
      "                temperature FLOAT,\n",
      "                wildfires   INTEGER,\n",
      "                droughts    FLOAT\n",
      "            );\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to FIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Insert['fips1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            INSERT INTO public.fips (fips_code, fips_name)\n",
      "            SELECT\n",
      "                DISTINCT CAST(FIPS_CODE AS INTEGER),\n",
      "                FIPS_NAME\n",
      "            FROM\n",
      "                public.stage_wildfires;\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Insert['fips2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            INSERT INTO public.fips (fips_code, fips_name)\n",
      "            SELECT\n",
      "                DISTINCT CAST(fips AS INTEGER),\n",
      "                CONCAT(CONCAT(COUNTY, '-'), STATE)\n",
      "            FROM\n",
      "                public.stage_droughts;\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to States Abbreviations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Insert['states_abbrv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            INSERT INTO public.states_abbrv (state, abbr) VALUES\n",
      "                ('Alabama', 'AL'),\n",
      "                ('Alaska', 'AK'),\n",
      "                ('Arizona', 'AZ'),\n",
      "                ('Arkansas', 'AR'),\n",
      "                ('California', 'CA'),\n",
      "                ('Colorado', 'CO'),\n",
      "                ('Connecticut', 'CT'),\n",
      "                ('Delaware', 'DE'),\n",
      "                ('Florida', 'FL'),\n",
      "                ('Georgia', 'GA'),\n",
      "                ('Hawaii', 'HI'),\n",
      "                ('Idaho', 'ID'),\n",
      "                ('Illinois', 'IL'),\n",
      "                ('Indiana', 'IN'),\n",
      "                ('Iowa', 'IA'),\n",
      "                ('Kansas', 'KS'),\n",
      "                ('Kentucky', 'KY'),\n",
      "                ('Louisiana', 'LA'),\n",
      "                ('Maine', 'ME'),\n",
      "                ('Maryland', 'MD'),\n",
      "                ('Massachusetts', 'MA'),\n",
      "                ('Michigan', 'MI'),\n",
      "                ('Minnesota', 'MN'),\n",
      "                ('Mississippi', 'MS'),\n",
      "                ('Missouri', 'MO'),\n",
      "                ('Montana', 'MT'),\n",
      "                ('Nebraska', 'NE'),\n",
      "                ('Nevada', 'NV'),\n",
      "                ('New Hampshire', 'NH'),\n",
      "                ('New Jersey', 'NJ'),\n",
      "                ('New Mexico', 'NM'),\n",
      "                ('New York', 'NY'),\n",
      "                ('North Carolina', 'NC'),\n",
      "                ('North Dakota', 'ND'),\n",
      "                ('Ohio', 'OH'),\n",
      "                ('Oklahoma', 'OK'),\n",
      "                ('Oregon', 'OR'),\n",
      "                ('Pennsylvania', 'PA'),\n",
      "                ('Rhode Island', 'RI'),\n",
      "                ('South Carolina', 'SC'),\n",
      "                ('South Dakota', 'SD'),\n",
      "                ('Tennessee', 'TN'),\n",
      "                ('Texas', 'TX'),\n",
      "                ('Utah', 'UT'),\n",
      "                ('Vermont', 'VT'),\n",
      "                ('Virginia', 'VA'),\n",
      "                ('Washington', 'WA'),\n",
      "                ('West Virginia', 'WV'),\n",
      "                ('Wisconsin', 'WI'),\n",
      "                ('Wyoming', 'WY'),\n",
      "                ('District of  Columbia', 'DC'),\n",
      "                ('Marshall Islands', 'MH');\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to States and Counties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Insert['states_counties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            INSERT INTO public.states_counties (state, county)\n",
      "            SELECT\n",
      "                DISTINCT state,\n",
      "                county\n",
      "            FROM\n",
      "                stage_wildfires;\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to Wildfires "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Insert['wildfires']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            INSERT INTO public.wildfires(\n",
      "                OBJECTID, FOD_ID, FPA_ID, FIRE_YEAR, DISCOVERY_DOY, DISCOVERY_TIME, STAT_CAUSE_CODE, STAT_CAUSE_DESCR, CONT_DOY, CONT_TIME, FIRE_SIZE, FIRE_SIZE_CLASS, STATE_COUNTY, FIPS_ID, DISCOVERY_DATE, CONT_DATE\n",
      "            )\n",
      "            WITH\n",
      "                FIPS_ID as (SELECT * FROM public.fips),\n",
      "                STATES_COUNTIES_ID as (SELECT * FROM public.states_counties)\n",
      "            SELECT \n",
      "                OBJECTID,\n",
      "                FOD_ID,\n",
      "                FPA_ID,\n",
      "                CAST(FIRE_YEAR AS INTEGER),\n",
      "                CAST(DISCOVERY_DOY AS INTEGER),\n",
      "                f_my_cast(DISCOVERY_TIME), \n",
      "                CAST(STAT_CAUSE_CODE AS INTEGER),\n",
      "                STAT_CAUSE_DESCR,\n",
      "                f_my_cast(CONT_DOY), \n",
      "                f_my_cast(CONT_TIME),\n",
      "                CAST(FIRE_SIZE AS FLOAT),\n",
      "                FIRE_SIZE_CLASS,\n",
      "                FIPS_ID.index,\n",
      "                STATES_COUNTIES_ID.index,\n",
      "                CAST(DISCOVERY_DATE_converted AS DATE),\n",
      "                CAST(CONT_DATE_converted AS DATE)\n",
      "            FROM\n",
      "                public.stage_wildfires,\n",
      "                FIPS_ID,\n",
      "                STATES_COUNTIES_ID\n",
      "            WHERE\n",
      "                FIPS_ID.fips_code = CAST(stage_wildfires.FIPS_CODE AS INTEGER) AND \n",
      "                FIPS_ID.fips_name = stage_wildfires.FIPS_NAME AND\n",
      "                STATES_COUNTIES_ID.state = stage_wildfires.state AND\n",
      "                STATES_COUNTIES_ID.county = stage_wildfires.county;\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to Temperatures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Insert['temperatures']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            INSERT INTO public.temperatures (dt, State, AverageTemperature)\n",
      "            SELECT\n",
      "                CAST(dt AS DATE),\n",
      "                State,\n",
      "                AverageTemperature_imputed\n",
      "            FROM\n",
      "                public.stage_temperatures;\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to Droughts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Insert['droughts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            INSERT INTO public.droughts(\n",
      "                releaseDate, FIPS_ID, STATE_COUNTY, None, D0, D1, D2, D3, D4, validStart, validEnd\n",
      "            )\n",
      "            WITH\n",
      "                FIPS_ID as (SELECT * FROM public.fips),\n",
      "                STATES_COUNTIES_ID as (SELECT * FROM public.states_counties)\n",
      "            SELECT\n",
      "                CAST(releaseDate AS DATE),\n",
      "                FIPS_ID.index,\n",
      "                STATES_COUNTIES_ID.index,\n",
      "                NONE,\n",
      "                D0,\n",
      "                D1,\n",
      "                D2,\n",
      "                D3,\n",
      "                D4,\n",
      "                CAST(validStart AS DATE),\n",
      "                CAST(validEnd AS DATE)\n",
      "            FROM\n",
      "                public.stage_droughts,\n",
      "                FIPS_ID,\n",
      "                STATES_COUNTIES_ID\n",
      "            WHERE\n",
      "                FIPS_ID.fips_code = CAST(stage_droughts.fips AS INTEGER) AND \n",
      "                FIPS_ID.fips_name = CONCAT(CONCAT(stage_droughts.COUNTY, '-'), stage_droughts.STATE) AND\n",
      "                STATES_COUNTIES_ID.state = stage_droughts.state AND\n",
      "                STATES_COUNTIES_ID.county = stage_droughts.county_cleaned;\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to Airquality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Insert['airquality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            -- we need to resolve the \"State\" column from staging table \"stage_airquality\"\n",
      "            -- into an abbreviation. that's where the table \"states_abbrv\" comes in!\n",
      "            INSERT INTO public.airquality (\n",
      "                STATE_COUNTY, city_name, aqi, date_of_last_change\n",
      "            )\n",
      "            WITH\n",
      "                STATES_ABBRV AS (SELECT * FROM public.states_abbrv),\n",
      "                STATES_COUNTIES_ID AS (SELECT * FROM public.states_counties)\n",
      "            SELECT\n",
      "              STATES_COUNTIES_ID.index,\n",
      "              public.stage_airquality.city_name,\n",
      "              public.stage_airquality.aqi,\n",
      "              CAST(public.stage_airquality.date_of_last_change AS DATE)\n",
      "            FROM\n",
      "                public.stage_airquality,\n",
      "                STATES_COUNTIES_ID,\n",
      "                STATES_ABBRV\n",
      "            WHERE\n",
      "                STATES_ABBRV.state = public.stage_airquality.state_name AND\n",
      "                STATES_COUNTIES_ID.state = \tSTATES_ABBRV.abbr  AND\n",
      "                STATES_COUNTIES_ID.county = public.stage_airquality.county_name;\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to Annual Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a fact table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = redshift.Redshift.Insert['annual_reports']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            INSERT INTO public.annual_reports (state, year, aqi, temperature, wildfires, droughts)\n",
      "            WITH states_counties_index AS \n",
      "            (\n",
      "                SELECT\n",
      "                    * \n",
      "                FROM\n",
      "                    states_counties \n",
      "            )\n",
      "            ,\n",
      "            states_abbrv AS \n",
      "            (\n",
      "                SELECT\n",
      "                    * \n",
      "                FROM\n",
      "                    states_abbrv \n",
      "            )\n",
      "            ,\n",
      "            yearly_aqi AS\n",
      "            (\n",
      "                SELECT\n",
      "                    SC_IDX.state,\n",
      "                    DATE_PART_YEAR(AQI.date_of_last_change) AS year,\n",
      "                    AVG(AQI.aqi) AS AVG_AQ \n",
      "                FROM\n",
      "                    airquality AS AQI,\n",
      "                    states_counties_index AS SC_IDX \n",
      "                WHERE\n",
      "                    SC_IDX.index = AQI.STATE_COUNTY \n",
      "                GROUP BY\n",
      "                    SC_IDX.state,\n",
      "                    year \n",
      "            )\n",
      "            ,\n",
      "            yearly_temperatures AS \n",
      "            (\n",
      "                SELECT\n",
      "                    states_abbrv.abbr AS state_abbreviated,\n",
      "                    DATE_PART_YEAR(dt) AS year,\n",
      "                    AVG(averagetemperature) AS avg_temp \n",
      "                FROM\n",
      "                    temperatures,\n",
      "                    states_abbrv \n",
      "                WHERE\n",
      "                    states_abbrv.state = temperatures.state \n",
      "                GROUP BY\n",
      "                    state_abbreviated,\n",
      "                    year \n",
      "                ORDER BY\n",
      "                    state_abbreviated,\n",
      "                    year \n",
      "            )\n",
      "            ,\n",
      "            yearly_wildfires AS \n",
      "            (\n",
      "                SELECT\n",
      "                    SC_IDX.state AS state,\n",
      "                    FIRE_YEAR,\n",
      "                    COUNT(*) AS total \n",
      "                FROM\n",
      "                    wildfires AS WF,\n",
      "                    states_counties_index AS SC_IDX \n",
      "                WHERE\n",
      "                    SC_IDX.index = WF.STATE_COUNTY \n",
      "                GROUP BY\n",
      "                    SC_IDX.state,\n",
      "                    FIRE_YEAR \n",
      "                ORDER BY\n",
      "                    SC_IDX.state ASC,\n",
      "                    FIRE_YEAR ASC \n",
      "            )\n",
      "            ,\n",
      "            yearly_droughts AS \n",
      "            (\n",
      "                SELECT\n",
      "                    SC_IDX.state,\n",
      "                    DATE_PART_YEAR(releasedate) AS year,\n",
      "                    AVG(GREATEST(NONE, D0, D1, D2, D3, D4)) AS avg_drought_coeff \n",
      "                FROM\n",
      "                    droughts,\n",
      "                    states_counties_index AS SC_IDX \n",
      "                WHERE\n",
      "                    SC_IDX.index = droughts.STATE_COUNTY \n",
      "                GROUP BY\n",
      "                    SC_IDX.state,\n",
      "                    year \n",
      "                ORDER BY\n",
      "                    SC_IDX.state ASC,\n",
      "                    year ASC \n",
      "            )\n",
      "            SELECT\n",
      "                yearly_aqi.state,\n",
      "                yearly_aqi.year,\n",
      "                yearly_aqi.avg_aq,\n",
      "                yearly_temperatures.avg_temp,\n",
      "                yearly_wildfires.total,\n",
      "                yearly_droughts.avg_drought_coeff \n",
      "            FROM\n",
      "                yearly_aqi \n",
      "                JOIN\n",
      "                    yearly_temperatures \n",
      "                    ON yearly_aqi.state = yearly_temperatures.state_abbreviated \n",
      "                    AND yearly_aqi.year = yearly_temperatures.year \n",
      "                JOIN\n",
      "                    yearly_wildfires \n",
      "                    ON yearly_aqi.state = yearly_wildfires.state \n",
      "                    AND yearly_aqi.year = yearly_wildfires.FIRE_YEAR \n",
      "                JOIN\n",
      "                    yearly_droughts \n",
      "                    ON yearly_aqi.state = yearly_droughts.state \n",
      "                    AND yearly_aqi.year = yearly_droughts.year \n",
      "            ORDER BY\n",
      "                yearly_aqi.state ASC,\n",
      "                yearly_aqi.year ASC\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "run_ddl_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
